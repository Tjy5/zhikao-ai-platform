#!/usr/bin/env python3
"""
æµ‹è¯•æ–‡æœ¬æ¸…ç†åŠŸèƒ½
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.text_sanitizer import TextSanitizer

def test_sanitizer():
    """æµ‹è¯•æ–‡æœ¬æ¸…ç†åŠŸèƒ½"""
    
    # æµ‹è¯•æ•°æ®
    test_texts = [
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«ä¸€äº›ç‰¹æ®Šå­—ç¬¦ï¼š\n\r\t",
        "å¦ä¸€ä¸ªæµ‹è¯•ï¼ŒåŒ…å«å¤šä½™ç©ºæ ¼  å’Œ   æ ‡ç‚¹ç¬¦å·ï¼Œï¼Œï¼Œï¼ï¼",
        "æµ‹è¯•Unicodeå­—ç¬¦ï¼šğŸ“âœ…âŒ",
        "æµ‹è¯•HTMLæ ‡ç­¾ï¼š<div>å†…å®¹</div>",
        "æµ‹è¯•URLï¼šhttps://example.com/path",
        "æµ‹è¯•é‚®ç®±ï¼šuser@example.com",
        "æµ‹è¯•æ•°å­—ï¼š123-456-7890",
    ]
    
    sanitizer = TextSanitizer()
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ–‡æœ¬æ¸…ç†åŠŸèƒ½...")
    print("=" * 50)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\næµ‹è¯• {i}:")
        print(f"åŸå§‹æ–‡æœ¬: {repr(text)}")
        
        cleaned = sanitizer.sanitize(text)
        print(f"æ¸…ç†å: {repr(cleaned)}")
        
        # æµ‹è¯•åˆ†è¯
        tokens = sanitizer.tokenize(cleaned)
        print(f"åˆ†è¯ç»“æœ: {tokens}")
        
        print("-" * 30)
    
    print("âœ… æ–‡æœ¬æ¸…ç†åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_sanitizer()